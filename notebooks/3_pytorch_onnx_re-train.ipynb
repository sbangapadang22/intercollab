{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch and ONNX Flow for Training\n",
    "\n",
    "\n",
    "## Goals\n",
    "\n",
    "* Learn how to re-training a model using Pytorch\n",
    "\n",
    "* Learn how to export a trained model to ONNX\n",
    "\n",
    "* Learn how to quantize an ONNX model to run inference on the NPU\n",
    "\n",
    "## References\n",
    "\n",
    "**[Ryzen AI Software Platform](https://ryzenai.docs.amd.com/en/latest/getstartex.html)**\n",
    "\n",
    "**[Vitis AI Execution Provider](https://onnxruntime.ai/docs/execution-providers/Vitis-AI-ExecutionProvider.html)**\n",
    "\n",
    "**[CIFAR10](https://github.com/EN10/CIFAR)**\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-box alert-warning\"> \n",
    "    \n",
    "Running this re-training notebook will generate model files that will overwrite the existing trained quantized file in the `onnx` folder.\n",
    "\n",
    "Please make sure you rename any existing model files in the `onnx` folder to save them.\n",
    "\n",
    "The names of the model files that will be written are the following:\n",
    "\n",
    "1. The trained ResNet-50 model on the CIFAR-10 dataset is: `onnx\\resnet_trained_for_cifar10.pt`.\n",
    "2. The trained ResNet-50 model on the CIFAR-10 dataset in ONNX format is: `onnx\\resnet_trained_for_cifar10.onnx`.\n",
    "3. The trained quantized ResNet-50 model on the CIFAR-10 dataset in ONNX format is: `onnx/resnet.qdq.U8S8.onnx`\n",
    "</div>   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import ResNet50_Weights, resnet50\n",
    "from torchvision.datasets import CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\n",
      "[QUARK-INFO]: Custom Op compilation start.\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: The custom_op already exists.\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Custom Op compilation already complete.\u001b[0m\n",
      "C:\\ProgramData\\anaconda3\\envs\\ryzen-ai-1.3.1\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "import onnxruntime\n",
    "from onnxruntime.quantization import CalibrationDataReader, QuantType, QuantFormat, CalibrationMethod, quantize_static\n",
    "\n",
    "from quark.onnx.quantization.config import (Config, get_default_config)\n",
    "from quark.onnx import ModelQuantizer\n",
    "from onnxruntime.quantization import CalibrationDataReader, QuantType, QuantFormat\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Prepare the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us retrain the [ResNet-50 model](https://arxiv.org/pdf/1512.03385.pdf) from PyTorch Hub using the CIFAR-10 dataset.\n",
    "\n",
    "The CIFAR-10 dataset is used to retrain the default model using the [transfer learning technique](https://www.youtube.com/watch?v=BqqfQnyjmgg&list=PLo2EIpI_JMQtNtKNFFSMNIZwspj8H7-sQ&index=3).   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-box alert-warning\">\n",
    "Make sure that the CIFAR-10 dataset is downloaded. For steps refer to the previous notebook.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model for re-training using transfer learning\n",
    "\n",
    "The pre-trained ResNet-50 model trained on 1,000 class ImageNet dataset by default has fully connected (FC) layer of output size 1,000. This means that it produces a 1,000-dimensional vector, where each dimension corresponds to a class in the ImageNet dataset.\n",
    "\n",
    "We use transfer learning to select a set of pre-trained weights for the model and then customize the model's classifier by replacing its FC layers. The modification includes adding two linear layers, one with 2,048 input features and 64 output features, followed by a ReLU activation function, and another linear layer with 64 input features and 10 output features. This adaptation transforms the ResNet-50 model into a classifier suitable for a specific task with 10 classes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# License 1 (see end of notebook)\n",
    "\n",
    "def load_resnet_model():\n",
    "    weights = ResNet50_Weights.DEFAULT\n",
    "    resnet = resnet50(weights=weights)\n",
    "    resnet.fc = torch.nn.Sequential(torch.nn.Linear(2048, 64), torch.nn.ReLU(inplace=True), torch.nn.Linear(64, 10))\n",
    "    return resnet\n",
    "\n",
    "\n",
    "# For updating learning rate\n",
    "def update_lr(optimizer, lr):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group[\"lr\"] = lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model re-training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the CIFAR-10 dataset directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "global models_dir, data_dir\n",
    "models_dir = \".\\\\onnx\"\n",
    "data_dir= \".\\\\onnx\\\\data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training process runs over 500 images with a `batch_size` of 100, i.e., over the total 50,000 images in the train set.\n",
    "\n",
    "The training process takes approximately 10 minutes to complete each epoch. Number of epochs can be varied to optimize the accuracy of the model.\n",
    "\n",
    "At the end of this process, we will save the trained model as an ONNX model and then we will also quantize this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# License 1 (see end of notebook)\n",
    "\n",
    "def prepare_model(num_epochs=0):\n",
    "    # Seed everything to 0\n",
    "    random.seed(0)\n",
    "    torch.manual_seed(0)\n",
    "    torch.cuda.manual_seed(0)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Hyper-parameters\n",
    "    num_epochs = num_epochs\n",
    "    learning_rate = 0.001\n",
    "\n",
    "    # Image preprocessing modules\n",
    "    transform = transforms.Compose(\n",
    "        [transforms.Pad(4), transforms.RandomHorizontalFlip(), transforms.RandomCrop(32), transforms.ToTensor()]\n",
    "    )\n",
    "\n",
    "    # CIFAR-10 dataset\n",
    "    train_dataset = torchvision.datasets.CIFAR10(root=data_dir, train=True, transform=transform, download=False)\n",
    "    test_dataset = torchvision.datasets.CIFAR10(root=data_dir, train=False, transform=transforms.ToTensor())\n",
    "\n",
    "    # Data loader\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=100, shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=100, shuffle=False)\n",
    "\n",
    "    model = load_resnet_model().to(device)\n",
    "\n",
    "    # Loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Train the model\n",
    "    total_step = len(train_loader)\n",
    "    curr_lr = learning_rate\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (images, labels) in enumerate(train_loader):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if (i + 1) % 100 == 0:\n",
    "                print(\n",
    "                    \"Epoch [{}/{}], Step [{}/{}] Loss: {:.4f}\".format(\n",
    "                        epoch + 1, num_epochs, i + 1, total_step, loss.item()\n",
    "                    )\n",
    "                )\n",
    "        # Decay learning rate\n",
    "        if (epoch + 1) % 20 == 0:\n",
    "            curr_lr /= 3\n",
    "            update_lr(optimizer, curr_lr)\n",
    "\n",
    "    # Test the model\n",
    "    model.eval()\n",
    "    if num_epochs:\n",
    "        with torch.no_grad():\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for images, labels in test_loader:\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "            accuracy = 100 * correct / total\n",
    "            print(\"Accuracy of the model on the test images: {} %\".format(accuracy))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet50-11ad3fa6.pth\" to C:\\Users\\aup-a/.cache\\torch\\hub\\checkpoints\\resnet50-11ad3fa6.pth\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 97.8M/97.8M [00:05<00:00, 19.2MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Step [100/500] Loss: 1.0639\n",
      "Epoch [1/1], Step [200/500] Loss: 0.9532\n",
      "Epoch [1/1], Step [300/500] Loss: 0.6795\n",
      "Epoch [1/1], Step [400/500] Loss: 0.5647\n",
      "Epoch [1/1], Step [500/500] Loss: 0.7307\n",
      "Accuracy of the model on the test images: 75.29 %\n"
     ]
    }
   ],
   "source": [
    "# Run training\n",
    "model = prepare_model(num_epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the trained Pytorch model by running the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(\"cpu\")\n",
    "model_path = f\"{models_dir}/resnet_trained_for_cifar10.pt\"\n",
    "torch.save(model, model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After completing the training process, observe the following output:   \n",
    "\n",
    "* The trained ResNet-50 model on the CIFAR-10 dataset is saved at the following location: `onnx/resnet_trained_for_cifar10.pt`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Convert Model to ONNX Format\n",
    "\n",
    "Run the following cell to save the trained model as an ONNX model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_onnx_model(model):\n",
    "    dummy_inputs = torch.randn(1, 3, 32, 32)\n",
    "    input_names = ['input']\n",
    "    output_names = ['output']\n",
    "    dynamic_axes = {'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}}\n",
    "    onnx_model_path = f\"{models_dir}/resnet_trained_for_cifar10.onnx\"\n",
    "    torch.onnx.export(\n",
    "        model,\n",
    "        dummy_inputs,\n",
    "        onnx_model_path,\n",
    "        export_params=True,\n",
    "        opset_version=13,\n",
    "        input_names=input_names,\n",
    "        output_names=output_names,\n",
    "        dynamic_axes=dynamic_axes,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save model\n",
    "save_onnx_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After completing this process, observe the following output:\n",
    "\n",
    "* The trained ResNet-50 model on the CIFAR-10 dataset is saved at the following location in ONNX format: `onnx/resnet_trained_for_cifar10.onnx`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the ONNX model\n",
    "\n",
    "Generated and adapted using Netron\n",
    ">Netron is a viewer for neural network, deep learning and machine learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-box alert-warning\">\n",
    "\n",
    "<strong>Note</strong> this is an image of the default model we are using. If you have modified or re-trained your model, please visit [Netron](https://netron.app/) to generate a graph for your model.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"800\"\n",
       "            height=\"600\"\n",
       "            src=\"https://netron.app/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x1f09616fe80>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import IFrame\n",
    "\n",
    "notebook_url = \"https://netron.app/\"\n",
    "\n",
    "iframe = IFrame(notebook_url, width=800, height=600)\n",
    "\n",
    "display(iframe)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Quantize the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quantizing AI models from floating-point to 8-bit integers reduces computational power and the memory footprint required for inference. For model quantization, you can either use [AMD Quark](https://quark.docs.amd.com/latest/index.html) or [Microsoft Olive](https://ryzenai.docs.amd.com/en/latest/olive_quant.html). This example utilizes the AMD Quark quantizer workflow. \n",
    "   \n",
    "This will generate a quantized model using QDQ quant format and UInt8 activation type and Int8 weight type. After the run is completed, the quantized ONNX model `resnet.qdq.U8S8.onnx` is saved to `onnx/resnet.qdq.U8S8.onnx`.\n",
    "    \n",
    "For more information on representation of quantized ONNX models (e.g., QDQ quant format, UInt8 activation type and Int8 weight type) see [here](https://onnxruntime.ai/docs/performance/model-optimizations/quantization.html#onnx-quantization-representation-format)   \n",
    "   \n",
    "The  ```QuantizationConfig``` class is used to configure the quantization parameters to the model. \n",
    "\n",
    "```python\n",
    "from quark.onnx import ModelQuantizer, PowerOfTwoMethod, QuantType\n",
    "from quark.onnx.quantization.config.config import Config, QuantizationConfig\n",
    "\n",
    "quant_config = QuantizationConfig(\n",
    "    quant_format=quark.onnx.QuantFormat.QDQ,\n",
    "    calibrate_method=quark.onnx.PowerOfTwoMethod.MinMSE,\n",
    "    input_nodes=[],\n",
    "    output_nodes=[],\n",
    "    op_types_to_quantize=[],\n",
    "    per_channel=False,\n",
    "    reduce_range=False,\n",
    "    activation_type=quark.onnx.QuantType.QInt8,\n",
    "    weight_type=quark.onnx.QuantType.QInt8,\n",
    "    nodes_to_quantize=[],\n",
    "    nodes_to_exclude=[],\n",
    "    subgraphs_to_exclude=[],\n",
    "    optimize_model=True,\n",
    "    use_dynamic_quant=False,\n",
    "    use_external_data_format=False,\n",
    "    execution_providers=['CPUExecutionProvider'],\n",
    "    enable_npu_cnn=False,\n",
    "    enable_npu_transformer=False,\n",
    "    convert_fp16_to_fp32=False,\n",
    "    convert_nchw_to_nhwc=False,\n",
    "    include_cle=False,\n",
    "    include_sq=False,\n",
    "    extra_options={},)\n",
    "config = Config(global_quant_config=quant_config)\n",
    "\n",
    "quantizer = ModelQuantizer(config)\n",
    "quantizer.quantize_model(input_model_path, output_model_path, calibration_data_reader=None)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell to define the calibration data reader (`resnet_calibration_reader`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# License 2 (see end of notebook)\n",
    "\n",
    "class CIFAR10DataSet:\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_dir,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.train_path = data_dir\n",
    "        self.vld_path = data_dir\n",
    "        self.setup(\"fit\")\n",
    "\n",
    "    def setup(self, stage: str):\n",
    "        transform = transforms.Compose(\n",
    "            [transforms.Pad(4), transforms.RandomHorizontalFlip(), transforms.RandomCrop(32), transforms.ToTensor()]\n",
    "        )\n",
    "        self.train_dataset = CIFAR10(root=self.train_path, train=True, transform=transform, download=False)\n",
    "        self.val_dataset = CIFAR10(root=self.vld_path, train=True, transform=transform, download=False)\n",
    "\n",
    "\n",
    "class PytorchResNetDataset(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sample = self.dataset[index]\n",
    "        input_data = sample[0]\n",
    "        label = sample[1]\n",
    "        return input_data, label\n",
    "\n",
    "\n",
    "def create_dataloader(data_dir, batch_size):\n",
    "    cifar10_dataset = CIFAR10DataSet(data_dir)\n",
    "    _, val_set = torch.utils.data.random_split(cifar10_dataset.val_dataset, [49000, 1000])\n",
    "    benchmark_dataloader = DataLoader(PytorchResNetDataset(val_set), batch_size=batch_size, drop_last=True)\n",
    "    return benchmark_dataloader\n",
    "\n",
    "\n",
    "class ResnetCalibrationDataReader(CalibrationDataReader):\n",
    "    def __init__(self, data_dir: str, batch_size: int = 16):\n",
    "        super().__init__()\n",
    "        self.iterator = iter(create_dataloader(data_dir, batch_size))\n",
    "\n",
    "    def get_next(self) -> dict:\n",
    "        try:\n",
    "            images, labels = next(self.iterator)\n",
    "            return {\"input\": images.numpy()}\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "\n",
    "def resnet_calibration_reader(data_dir, batch_size=16):\n",
    "    return ResnetCalibrationDataReader(data_dir, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell to quantize and save the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\n",
      "[QUARK-INFO]: The input ONNX model onnx/resnet_trained_for_cifar10.onnx can create InferenceSession successfully\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The configuration for quantization is Config(global_quant_config=QuantizationConfig(calibrate_method=<PowerOfTwoMethod.MinMSE: 1>, quant_format=<QuantFormat.QDQ: 1>, activation_type=<QuantType.QUInt8: 1>, weight_type=<QuantType.QInt8: 0>, input_nodes=[], output_nodes=[], op_types_to_quantize=[], nodes_to_quantize=[], extra_op_types_to_quantize=[], nodes_to_exclude=[], specific_tensor_precision=False, execution_providers=['CPUExecutionProvider'], per_channel=False, reduce_range=False, optimize_model=True, use_dynamic_quant=False, use_external_data_format=False, convert_fp16_to_fp32=False, convert_nchw_to_nhwc=False, include_sq=False, include_cle=False, include_auto_mp=False, include_fast_ft=False, enable_npu_cnn=True, enable_npu_transformer=False, debug_mode=False, print_summary=True, ignore_warnings=True, log_severity_level=1, extra_options={'ActivationSymmetric': True}))\n",
      "[QUARK_INFO]: Time information:\n",
      "2025-02-25 16:51:18.617728\n",
      "[QUARK_INFO]: OS and CPU information:\n",
      "                                        system --- Windows\n",
      "                                          node --- AUP11\n",
      "                                       release --- 10\n",
      "                                       version --- 10.0.26100\n",
      "                                       machine --- AMD64\n",
      "                                     processor --- AMD64 Family 26 Model 36 Stepping 0, AuthenticAMD\n",
      "[QUARK_INFO]: Tools version information:\n",
      "                                        python --- 3.10.16\n",
      "                                          onnx --- 1.16.1\n",
      "                                   onnxruntime --- 1.19.0\n",
      "                                    quark.onnx --- 0.6.0+dba9ca364\n",
      "[QUARK_INFO]: Quantized Configuration information:\n",
      "                                   model_input --- onnx/resnet_trained_for_cifar10.onnx\n",
      "                                  model_output --- onnx/resnet.qdq.U8S8.onnx\n",
      "                       calibration_data_reader --- <__main__.ResnetCalibrationDataReader object at 0x0000026CDE334910>\n",
      "                         calibration_data_path --- None\n",
      "                                  quant_format --- QDQ\n",
      "                                   input_nodes --- []\n",
      "                                  output_nodes --- []\n",
      "                          op_types_to_quantize --- []\n",
      "                    extra_op_types_to_quantize --- []\n",
      "                                   per_channel --- False\n",
      "                                  reduce_range --- False\n",
      "                               activation_type --- QUInt8\n",
      "                                   weight_type --- QInt8\n",
      "                             nodes_to_quantize --- []\n",
      "                              nodes_to_exclude --- []\n",
      "                                optimize_model --- True\n",
      "                      use_external_data_format --- False\n",
      "                              calibrate_method --- PowerOfTwoMethod.MinMSE\n",
      "                           execution_providers --- ['CPUExecutionProvider']\n",
      "                                enable_npu_cnn --- True\n",
      "                        enable_npu_transformer --- False\n",
      "                     specific_tensor_precision --- False\n",
      "                                    debug_mode --- False\n",
      "                          convert_fp16_to_fp32 --- False\n",
      "                          convert_nchw_to_nhwc --- False\n",
      "                                   include_cle --- False\n",
      "                                    include_sq --- False\n",
      "                               include_fast_ft --- False\n",
      "                                 extra_options --- {'ActivationSymmetric': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\n",
      "[QUARK-INFO]: Obtained calibration data with 62 iters\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Removed initializers from input\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Simplified model sucessfully\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Loading model...\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: The input ONNX model C:/Users/aup-a/AppData/Local/Temp/vai.simp.l08qdde9/model_simp.onnx can run inference successfully\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: optimize the model for better hardware compatibility.\u001b[0m\n",
      "\u001b[33m\n",
      "[QUARK-WARNING]: The opset version is 13 < 17. Skipping fusing layer normalization.\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Start calibration...\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Start collecting data, runtime depends on your model size and the number of calibration dataset.\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Finding optimal threshold for each tensor using PowerOfTwoMethod.MinMSE algorithm ...\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Use all calibration data to calculate min mse\u001b[0m\n",
      "Computing range: 100%|███████████████████████████████████████████████████████████| 125/125 [00:22<00:00,  5.56tensor/s]\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Finished the calibration of PowerOfTwoMethod.MinMSE which costs 24.8s\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Remove QuantizeLinear & DequantizeLinear on certain operations(such as conv-relu).\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Rescale GlobalAveragePool /avgpool/GlobalAveragePool with factor 1.0 to simulate DPU behavior.\u001b[0m\n",
      "\u001b[32m\n",
      "[QUARK-INFO]: Adjust the quantize info to meet the compiler constraints\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The operation types and their corresponding quantities of the input float model is shown in the table below.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type              </span>┃<span style=\"font-weight: bold\"> Float Model               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Conv                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 53                        </span>│\n",
       "│ Relu                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 50                        </span>│\n",
       "│ MaxPool              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Add                  │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 16                        </span>│\n",
       "│ GlobalAveragePool    │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Flatten              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 1                         </span>│\n",
       "│ Gemm                 │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> 2                         </span>│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> onnx/resnet.qdq.U8S8.onnx </span>│\n",
       "└──────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFloat Model              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ Conv                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m53                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Relu                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m50                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ MaxPool              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Add                  │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m16                       \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ GlobalAveragePool    │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Flatten              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m1                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gemm                 │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m2                        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "├──────────────────────┼───────────────────────────┤\n",
       "│ Quantized model path │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46monnx/resnet.qdq.U8S8.onnx\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└──────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The quantized information for all operation types is shown in the table below.\n",
      "The discrepancy between the operation types in the quantized model and the float model is due to the application of graph optimization.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Op Type           </span>┃<span style=\"font-weight: bold\"> Activation </span>┃<span style=\"font-weight: bold\"> Weights  </span>┃<span style=\"font-weight: bold\"> Bias     </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━┩\n",
       "│ Conv              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> UINT8(53)  </span>│<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> INT8(53) </span>│<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> INT8(53) </span>│\n",
       "│ MaxPool           │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> UINT8(1)   </span>│<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">          </span>│<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">          </span>│\n",
       "│ Add               │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> UINT8(16)  </span>│<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">          </span>│<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">          </span>│\n",
       "│ GlobalAveragePool │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> UINT8(1)   </span>│<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">          </span>│<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">          </span>│\n",
       "│ Flatten           │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> UINT8(1)   </span>│<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">          </span>│<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">          </span>│\n",
       "│ Gemm              │<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> UINT8(2)   </span>│<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> INT8(2)  </span>│<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\"> INT8(2)  </span>│\n",
       "└───────────────────┴────────────┴──────────┴──────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mOp Type          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mActivation\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mWeights \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mBias    \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━┩\n",
       "│ Conv              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mUINT8(53) \u001b[0m\u001b[1;38;5;46m \u001b[0m│\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mINT8(53)\u001b[0m\u001b[1;38;5;46m \u001b[0m│\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mINT8(53)\u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ MaxPool           │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mUINT8(1)  \u001b[0m\u001b[1;38;5;46m \u001b[0m│\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Add               │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mUINT8(16) \u001b[0m\u001b[1;38;5;46m \u001b[0m│\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ GlobalAveragePool │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mUINT8(1)  \u001b[0m\u001b[1;38;5;46m \u001b[0m│\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Flatten           │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mUINT8(1)  \u001b[0m\u001b[1;38;5;46m \u001b[0m│\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46m        \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "│ Gemm              │\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mUINT8(2)  \u001b[0m\u001b[1;38;5;46m \u001b[0m│\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mINT8(2) \u001b[0m\u001b[1;38;5;46m \u001b[0m│\u001b[1;38;5;46m \u001b[0m\u001b[1;38;5;46mINT8(2) \u001b[0m\u001b[1;38;5;46m \u001b[0m│\n",
       "└───────────────────┴────────────┴──────────┴──────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calibrated and quantized model saved at: onnx/resnet.qdq.U8S8.onnx\n"
     ]
    }
   ],
   "source": [
    "# License 2 (see end of notebook)\n",
    "\n",
    "# `input_model_path` is the path to the original, unquantized ONNX model.\n",
    "input_model_path = \"onnx/resnet_trained_for_cifar10.onnx\"\n",
    "\n",
    "# `output_model_path` is the path where the quantized model will be saved.\n",
    "output_model_path = \"onnx/resnet.qdq.U8S8.onnx\"\n",
    "\n",
    "# `calibration_dataset_path` is the path to the dataset used for calibration during quantization.\n",
    "calibration_dataset_path = \"onnx/data/\"\n",
    "\n",
    "# `dr` (Data Reader) is an instance of ResNet50DataReader, which is a utility class that \n",
    "# reads the calibration dataset and prepares it for the quantization process.\n",
    "dr = resnet_calibration_reader(calibration_dataset_path)\n",
    "\n",
    "#Quantization with Quark\n",
    "    \n",
    "# Get quantization configuration\n",
    "quant_config = get_default_config(\"XINT8\")\n",
    "config = Config(global_quant_config=quant_config)\n",
    "print(f\"The configuration for quantization is {config}\")\n",
    "\n",
    "# Create an ONNX quantizer\n",
    "quantizer = ModelQuantizer(config)\n",
    "\n",
    "# Quantize the ONNX model\n",
    "quantizer.quantize_model(input_model_path, output_model_path, dr)\n",
    "\n",
    "print('Calibrated and quantized model saved at:', output_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After completing the quantization process, observe the following output:\n",
    "\n",
    "* The quantized ResNet-50 model on the CIFAR-10 dataset is saved at the following location in ONNX format: `onnx/resnet.qdq.U8S8.onnx`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Deploy the Model on NPU for Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-box alert-warning\">\n",
    "\n",
    "To run Inference using the model generated in this notebook please refer to the [Pytorch_ONNX_Inference](5_1_pytorch_onnx_inference.ipynb) notebook.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Licenses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "License 1\n",
    "\n",
    "```python\n",
    "# -------------------------------------------------------------------------\n",
    "# Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "# Licensed under the MIT License.\n",
    "# --------------------------------------------------------------------------\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "License 2\n",
    "\n",
    "```python\n",
    "#################################################################################  \n",
    "# License\n",
    "# Ryzen AI is licensed under `MIT License <https://github.com/amd/ryzen-ai-documentation/blob/main/License>`_ . Refer to the `LICENSE File <https://github.com/amd/ryzen-ai-documentation/blob/main/License>`_ for the full license text and copyright notice.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "<center>\n",
    "Copyright&copy; 2023 AMD, Inc\n",
    "</center>\n",
    "<center>\n",
    "SPDX-License-Identifier: MIT\n",
    "</center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
